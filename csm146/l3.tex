\chapter{Decision Trees}

Consider the sample dataset below: 
\begin{center}
    \img{./img/decisiontreedataset.png}
\end{center}
What would be the label for a red triangle? We can construct a decision tree in order to determine the answer. 
\begin{center}
    \img{./img/decisiontree.png}
\end{center}
Based on this tree, we can see that a red triangle would be labeled B.

Many decisions can be modeled as tree structures in this way.

\section{Representation}
\begin{itemize}
    \item decision trees are classifiers for instances represented as feature vectors
    \item nodes are tests for feature values 
    \item we draw a branch for each value of a node's feature (for numerical values we use thresholds i.e. X > 100)
    \item leaves specify labels
\end{itemize}

We can use decision trees to express any boolean function. How do we learn the decision tree?

\section{Algorithm}
We recursively build the decision tree top down with funcion ID3(S, Attributes, Label) as follows:
\begin{itemize}
    \item base case: if all examples are labeled the same, then return a single node with the label
    \item let A = the attribute that "best" classifies S
    \item For each possible value v of A
    \begin{itemize}
        \item Add a new tree branch corresponding to A=v
        \item Let $S_v$ be the subset of examples in S w/ A = v
        \item if $S_v$ is empty, add leaf node with the common label in S
        \item otherwise, add the subtree generated by ID3($S_v$, Attributes - $\{A\}$, Label) at this branch
    \end{itemize}
\end{itemize}

How do we determine the "best" attribute to split? The goal is to create the smallest decision tree possible (which can be accomplished w/ a greedy heuristic, sometimes). The problem itself is NP-hard, but it is good enough to be optimal most of the time. 

\section{Information Gain}
Information gain is the heuristic described above. The idea is that gaining information will reduce uncertainty, which can be measured by entropy. The formal definition of entropy H for a set S is as follows
\[
    H[S]   = -P_+log_2(P_+) - P_-log_2(P_-)
\]
where $P_+$ and $P_-$ represent the proportin of positive and negative examples in S respectively.

More generally, if a random variable S has K different values, the entropy is given basicstyle
\[
    H[S] = - \sum_{v=1}^{K}P(S=a_v)log_2P(S=a_v)
\]

The information gain of an attribute is the expected reudction in entropy caused by partitioning on that attribute
\[
    Gain(S,A) = Entropy(S) - \sum_{v \in Values(a)}\frac{|S_v|}{|S|}Entropy(S_v)
\]