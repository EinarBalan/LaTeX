\chapter{Kernel Methods}

We saw in previous chapters that linear models such as perceptrons and logistic regression could not classify data that was not linearly separable with 100\% training accuracy. However, it may be possible to create some mapping for non-linearly separable data that makes it linearly separable, and thus learnable for linear models. Consider the image below. 

\begin{center}
    \img{./img/non-linear-data.png}
\end{center}

The data appears to not be separable linearly.  However, if we introduce a new feature $x^2$, we can separate it as follows.

\begin{center}
    \img{./img/non-linear-mapping.png}
\end{center}

We can use this idea to modify our perceptron algorithm to allow it to converge when data is not linearly separable in 2 dimensions. Here, $\phi(x) = [x, x^2]^T$.

Modified Perceptron Algorithm
\begin{itemize}
    \item Initialize $w \leftarrow 0 \in \mathbb{R}^{2n}$
    \item For (x, y) in D:
    \begin{itemize}
        \item if $yw^T \phi(x) \leq 0$
        \begin{itemize}
            \item $w \leftarrow w + y \phi(x)$
        \end{itemize}
    \end{itemize}
    \item return w
\end{itemize}

\vspace{1cm}
What if our mapping function is more complex i.e. mapping data to infinite dimensions? We can use the fact that the perceptron algorithm essentially just builds a linear combination throughout its run time to create a more specialized version of the algorithm. We can consider $w = \sum_{i = 1}^{m} \alpha_i y_i x_i$ where $\alpha_i$ is the number of mistakes the perceptron made on $x_i$.

Modified (Again) Perceptron Algorithm
\begin{itemize}
    \item Initialize $a \leftarrow 0 \in \mathbb{R}^{m}$
    \item For $(x_i, y_i)$ in D:
    \begin{itemize}
        \item if $y_i \sum_j \alpha_j y_j \phi(x_i)^T \phi(x_j) \leq 0$
        \begin{itemize}
            \item $\alpha_i \leftarrow \alpha_i + 1$
        \end{itemize}
    \end{itemize}
    \item return $\alpha$
\end{itemize}

We can compute the dot product $\phi(x_i)^T \phi(x_j)$ efficiently even at high dimensions. In fact, it scales at $O(D)$ where D is the number of dimensions. The \kw{Kernel Trick} allows us to save time/space by computing the value of our kernel function by performing operations in the original space (without a feature transformation!) 

\begin{definition}
    Kernel function

    A kernel function k satisifies 
    \begin{gather*}
        k(x_m, x_n)   = k(x_n, x_m) \\
        k(x_m, x_n) = \phi(x_m)^T \phi(x_n)
    \end{gather*}
    for some function $\phi$.
    
    For example, $(x_m^Tx_n)^2$ is a kernel because it is the linear product of the following mapping

    \begin{align*}
        \phi: x = \begin{pmatrix}
            x_1 \\
            x_2
        \end{pmatrix} \rightarrow \phi(x) = \begin{pmatrix}
            x_1^2 \\
            \sqrt{2}x_1x_2 \\
            x_2^2
        \end{pmatrix}
    \end{align*}
\end{definition}

\begin{example}
    For $x \in \mathbb{R}^2$, show $(4+9x_i^Tx_j)^2$ is a valid kernel.

    We need to show that $(4+9x_i^Tx_j)^2 = \phi(x_m)^T \phi(x_n)$ for some $\phi$.

    \begin{equation*}
        \begin{split}
            (4+9x_i^Tx_j)^2 &= (4+9x_{i,1}x_{j,1} + 9x_{i,2}x_{j,2})^2 \\
            &= (16 + 81x_{i,1}^2x_{j,1}^2 + 81x_{i,2}^2x_{j,2}^2 + 2 * 4 * 9x_{i,1}x_{j,1} + 2 * 9x_{i,1}x_{j,1} * 9x_{i,2}x_{j,2} + 2 * 9x_{i,2}x_{j,2} * 4) \\
            &= \begin{bmatrix}
                4 \\
                9x_{i,1}^2 \\
                9x_{i,2}^2 \\
                6\sqrt{2}x_{i,1} \\
                9\sqrt{2}x_{i,1}x_{i,2} \\
                6\sqrt{2}x_{i,2} \\
            \end{bmatrix}^T \begin{bmatrix}
                4 \\
                9x_{i,1}^2 \\
                9x_{j,2}^2 \\
                6\sqrt{2}x_{j,1} \\
                9\sqrt{2}x_{j,1}x_{j,2} \\
                6\sqrt{2}x_{j,2} \\
            \end{bmatrix}
        \end{split}
    \end{equation*}
    \qed{}

\end{example}

\begin{absolutelynopagebreak}
    We can have several types of kernels. We saw the polynomial kernel above. 
    \begin{itemize}
        \item Linear kernel: $x^Ty$
        \item Polynomial kernel: $(x^Ty + c)^d$
        \item RBF kernel (infinite dimensions): $\exp(-\frac{||x-x'||^2}{2\sigma^2})$
    \end{itemize}
\end{absolutelynopagebreak}

Moving from our modified Perceptron (Dual Perceptron) Algorithm above, we get to the kernel perceptron algorithm. Rather than compute the dot product at high dimension we save computations by computing the kernel function:

\kw{Kernel Perceptron Algorithm}:
\begin{itemize}
    \item Initialize $a \leftarrow 0 \in \mathbb{R}^{m}$
    \item For $(x_i, y_i)$ in D:
    \begin{itemize}
        \item if $y_i \sum_j \alpha_j y_j K(x_i, x_j) \leq 0$
        \begin{itemize}
            \item $\alpha_i \leftarrow \alpha_i + 1$
        \end{itemize}
    \end{itemize}
    \item return $\alpha$
\end{itemize}

\section{Support Vector Machines}
SVM's are similar to perceptrons except that they maximize the margin of the data set. Recall that the margin of a hyperplane for a dataset is the distance between the hyperplane nad the datapoint closest to it.

We will skip the derivation, but to maximize the margin we seek to minimize $\frac{1}{2}w^Tw$ such that $\forall i, y_i(w^Tx_i + b) \ge 1$. The second constraint does not allow for any training error. If the data is not seperable, there is no w that will classify the data. This is a bit much to ask. Can we do better? Instead of using this "hard" SVM, we can introduce a slack variable $\xi_i$ along with a penalty to allow for errors but attempt to minimize them. The precise optimzation problem becomes

\begin{gather*}
    min_{w, b, \xi_i}(\frac{1}{2}w_Tw + C\Sigma_i \xi_i) \\
    s.t. \forall i, y_i(w^Tx_i + b) \ge 1 - \xi_i \\
    (\xi_i \ge 0)
\end{gather*}

or equivalently (via algebraic manipulation)$: $

\begin{gather*}
    J^t(w) = min_{w, b, \xi_i}(\frac{1}{2}w_Tw + C\Sigma_i max(0, 1-y_i(w^Tx_i + b))) 
\end{gather*}

First term represents us maximizing the margin while the second term is the penalty for mispredictions (also known as the loss function; called the hinge loss). C is a hyperparameter that controls the tradeoff between a large margin and a small loss. 

In general,
\[
  min_{w\in \mathbb{R}^d} R(w) + C \Sigma_{(x, y) \in \hat{D}} [L(x,w,y)]
\]

We can use multiple kinds of loss:
\begin{itemize}
    \item 0-1 loss 
    \item hinge loss (which is seen above)
    \begin{itemize}
        \item preferable to 0-1 because more easily differentiable
    \end{itemize}
    \item logistic loss
    \item perceptron loss
\end{itemize}

There are also many choices for the regularization:
\begin{itemize}
    \item simply \# of non-zero elements in w (L0)
    \item $\sum_i |w_i|$ (L1)
    \item $\sum_i w_i^2$ (L2)
\end{itemize}

\subsection*{Training}
Note that the hinge loss is not directly differentiable. Instead, we must break it into subgradients.

\begin{equation*}
    \nabla J^t = \begin{cases}
        w & \text{if } max(0, 1-y_i(w^Tx_i+b)) = 0 \\
        w - Cy_ix_i & \text{otherwise}
    \end{cases}
\end{equation*}

We then must modify SGD to accommodate for the subgradient.

\begin{verbatim}
    Given a training set D
    Initialize w = 0
    For epoch 1..T:
        For (x, y) in D:
            if y(w^Tx + b) < 1:
                w = w - eta * (w - Cyx)
                b = b + eta * Cy
            else:
                w = w - eta * w
\end{verbatim}

SVM optimizes the hinge loss with L2 regularization (as opposed to Perceptron which optimizes perceptron loss with no regularization).

\begin{definition}
    Support Vector

    A support vector is a datapoint that contributes to learning the weights for an SVM. This is the case when the datapoint is misclassifed, or lies either within or on the margin.
\end{definition}