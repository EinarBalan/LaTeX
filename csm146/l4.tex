\chapter{Linear Models}

In linear models, a hyperplane separates the data space in order to classify the data. Data points on either side of the hyperplane are classified differently. In two dimensions, a line is drawn to separate the data.

\begin{center}
    \img{./img/linearmodel-data.png}
\end{center}

The hypothesis space consists of many potential linear models, some of which are better than others. In general, to be "better" means that the model separates the data better than other models.

We have two parameters for our linear model: w and b. w is a column vector representing how much each feature should be weighted and b is the bias.

\begin{center}
    \img{./img/linear-model-params.png}
\end{center}

We classify based on if $w^Tx + b > 0$ or $ < 0$.

\begin{example}
    If a line represented by $w^Tx + b = 0$ passes through (0, 1) and (3, 0), what are w and b?

    To solve we simply set up a system of equations over $w_1x_1 + w_2x_2 + b = 0$.

    \begin{align*}
        0x_1 + 1x_2 + b = 0 \\
        3x_1 + 0x_2 + b = 0
    \end{align*}
    \begin{align*}
        3x_1 - 1x_2 = 0 \\
        3x_1 = x_2
    \end{align*}

    So one solution is $w^T = [1, 3]$ and $b = -3$. Note that there are infinitely many solutions.
\end{example}

\begin{definition}
    Linear Models for Binary Classification

    Given training set $D = \{(x, y), x \in \mathbb{R}^d, y \in \{-1, 1\}$, we can learn a hypothesis function $h \in H$.
    \[
        H = \{h | h(x) = sgn(w^Tx + b)\}  
    \]
    where y = h(x) and sgn(z) outputs 1 if z >= 0 and -1 otherwise. 

    This can be modeled with the following picture:
    \begin{center}
        \img{./img/linear-class-diagram.png}
    \end{center}

    We can also add the bias term to the weight vector and a single 1 term at the end of x to reduce complexity of notation.
\end{definition}

See slides for an example of converting a decision tree to a linear model.

\section{Perceptrons}
To learn is to find the best parameters. In this case, w and b. There are several algorithms to do so such as the perceptron, logistic regression, linear support vector machines, and more. Different methods will define the "best parameters" in a different way. First we will cover perceptrons. 

The main idea behind the perceptron algorithm is to learn from mistakes. Concretely, it is the following:
\begin{itemize}
    \item For (x, y) in D:
    \item $\hat{y} = sgn(w^Tx)$
    \item if $\hat{y} \neq y, w \leftarrow w + yx$
\end{itemize}

Essentially, for a misprediction we are adjusting the decision boundary in order correct our mistake. After enough mistakes, we will converge (if possible).

We can expand on this idea by adding \kw{epochs}, or more iterations of the algorithm being performed on our data. We continue these epochs until the hyperplane converges. This is one hyperparameter. We can also add $\eta$ as a scaling factor to our adjustment term $yx$ (i.e. $w \leftarrow w + \eta yx$). This is another hyperparameter.

It is important to not that a linear model \kw{cannot} represent all functions. In fact, it can only separate linarly separable functions. As a consequence, functions like XOR cannot be learned. However, if a data is linearly separable, the perceptron algorithm will always converge (Convergence Theorem). The converge rate depends on the difficulty of the problem. If it is not, the algorithm will enter an infinite loop.

To quantify difficulty, lets dicuss margin.

\begin{definition}
    Margin

    If a hyperplane can separate the data, the margin of a hyperplane for a dataset is the distance between the hyperplane and the data point nearest to it.

    The margin of a dataset ($\gamma$) is the maximum margin possible for that dataset using any weight vector.

    We can quantify the difficulty with $\frac{R}{\gamma}$ where $\forall i,  ||x_i|| \le R $ ($x_i$ is a feature vector). The \kw{Mistake Bound Theorem} states that the perceptron algorithm will make at most $(\frac{R}{\gamma})^2$ mistakes on the training sequence. We can see that $\gamma$ and the number of mistakes are inversely proportional, meaning that a larger margin indicates an easier problem. Intuitively, this makes sense as the decision boundary does not need to be tuned as tightly if the margin is high, resulting in fewer mistakes being needed to tune the parameters. Note that the number of mistakes is not the same as the number of data points seen. 

Perceptrons are good when the data is linearly separable. Unfortunately, in the real world this is very often not the case. We will see ways to deal with this.
\end{definition}

\section{Logistic Regression}
As discussed, if the data is not linearly separable the perceptron algorithm will not converge. Another approach is to "model the probability" with logistic regression. Here the idea is to have discrete labels $y$, but to predict $P(y = 1 | x)$ rather than the label itself. In other words, we want to build a model h(x) where
\[
    h(x) = \sigma(w^Tx + b) \approx P(y = 1 | x)
\]  
and 
\[
    \sigma(x) = \frac{exp(x)}{1 + exp(x)} = \frac{1}{1+exp(-x)}
\]

\subsection*{Why Sigmoid?}
Probability by definition is a number between 0 and 1. We use the sigmoid function because it has a domain over $\mathbb{R}$ and a range between 0 and 1. That is, it maps all real numbers to (what looks like) a probability. Intuitively, our decision boundary will be where $\sigma(x) = \frac{1}{2}$ because the value is equally likely to be classifed either way (Note that right now we are discussing binary classification).

\subsection*{Training Logistic Regression}
We want to find the best w and b such that the predicted probabilities most closely align with their true labels.

\begin{definition}
    Maximum Likelihood Estimator

    Let $X_1, ..., X_N$ be independent and identically distributed with PDF $p(x|\theta)$. The likelihood function $L(\theta)$ is defined as 
    \[
        L(\theta) = p(X_1, ..., X_N | \theta) = \prod_{i=1}^{N}p(X_i | \theta)
    \]

    The maximum likelihood estimator $\hat{\theta}$ is the value of $\theta$ such that $L(\theta)$ is maximized.

    Concretely, the MLE is 
    $$  \text{argmax}_{w, b} \sum_{i=1}^{m}log(\sigma(y_i(w^Tx_i + b)))$$ (after some algebraic manipulation).
\end{definition}

We can train our model using the above idea. Essentially, we will find w and b by maximizing P(S | w, b). There is no closed form solution for this optimazation. Instead, we must apply another strategy. We will look at gradient descent.

\section{Gradient Descent}
\begin{center}
    \img{./img/gradient-descent.png}
\end{center}
We can solve the logistic regression optimzation and determine the maximum likelihood estimator using gradient descent. The intuition behind gradient descent is that we are looking for the best direction to reach a location. In this case, the best direction is given by the gradient and the location is the global minimum of our function. Concretely, the steps are as follows:
\begin{enumerate}
    \item start at a random point
    \item determine a descent direction
    \item choose a step size
    \item update
    \item repeat until stopping criterion is satisfied
\end{enumerate}
\img{./img/gradient-descent-alg.png}


Choosing the right step size $\eta$ is important. If it is too small it will take too long to reach a minimum, and if it is too large we will rubberband back and forth repeatedly.
\begin{center}
    \img{./img/gradient-descent-eta.png}
\end{center}

We can apply gradient descent in order to minimize our log likelihood function (AKA our loss function) in logistic regression. We define 

$$ L(w, b) = \text{arg min} \sum_{i=1}^{m}log(1+exp(-y_i(w^Tx_i + b)))$$

$L$ is our loss function. Recall that it comes from our model $h(x) = \sigma(w^Tx+b) \approx P(y = 1 | x)$ where $\sigma = \frac{1}{1+exp(-z)}$.

We can take its gradient via the following series of steps. 

\begin{align*}
    \nabla L(w, b) = \sum_{i=1}^{m} \nabla log(1+exp(-y_i(w^Tx_i + b))) \\
     = \sum_{i=1}^{m} \nabla log(\frac{1}{\sigma(y_i(w^Tx_i+b)}))
\end{align*}


We take the gradient of the reciprocal sigmoid function without yet applying the chain rule.
\begin{align*}
    \nabla log\frac{1}{\sigma(z)} = \nabla log(1+exp(-z)) = \frac{exp(-z)}{1+exp(-z)} \\
    = -\frac{1 + exp(-z) - 1}{1 + exp(-z)} = -1 + \frac{1}{1 + exp(-z)} \\
    = \sigma(z) - 1
\end{align*}

Now to apply chain rule.
\begin{align*}
    \nabla_w L(w, b) = \sum_{i=1}^m \nabla_w log\frac{1}{\sigma(y_i(w^Tx_i + b))} \\
    = \sum_{i=1}^m (\sigma(y_i(w^Tx_i +b)) - 1)y_ix_i \\
    \\
    \nabla_b L(w, b) = \sum_{i=1}^m (\sigma(y_i(w^Tx_i +b)) - 1)y_i \\
\end{align*}

This completes the derivation of our partial gradients. To apply them to our logistic regression, we use the following algorithm:

\vspace{.25cm}

\kw{Gradient Descent}
\begin{itemize}
    \item initialize w
    \item For every epoch
    \begin{itemize}
        \item compute $\nabla_wL(w,b)$ and $\nabla_bL(w,b)$
        \item $w \gets w - \eta \nabla_w L(w,b)$
        \item $b \gets b - \eta \nabla_b L(w,b)$
    \end{itemize}
    \item return w and b
\end{itemize}

\vspace{.25cm}

This algorithm can get quite expensive. Consider a large data set. We need to compute $\sigma(y_i(w^Tx_i + b)) - 1$ for every data point $(x_i, y_i)$. In addition, gradient descent typically requires many iterations to converge. This adds up to many operations that need to be performed. Can we do better?

\section*{Stochastic Gradient Descent}
Rather than compute the gradient at all points in our training set to do a single update, we use a single point in order to perform an update. These changes accumulate to approximate the overall gradient. So, in effect, the only difference is how frequently we update our parameters.

\vspace{.25cm}

\kw{Stochastic Gradient Descent}
\begin{itemize}
    \item initialize w
    \item For every epoch
    \begin{itemize}
        \item sample a data point $(x_i, y_i)$ from S
        \item compute $\nabla_wL_i(w,b)$ and $\nabla_bL_i(w,b)$
        \item $w \gets w - \eta \nabla_w L_i(w,b)$
        \item $b \gets b - \eta \nabla_b L_i(w,b)$
    \end{itemize}
    \item return w and b
\end{itemize}

\section*{Linear Regression}
Linear regression seeks to approximate a real value output y based on input x. It is essentially drawing a line of best fit for the data. We use Least Mean Squares Regression (LMS) to compute it.

$$ \text{argmin}_{w,b}\frac{1}{2} \sum_{i=1}^m (y_i - (w^Tx_i + b))^2 $$

As the name suggests, we are finding the parameters that minimize the squares of the difference of the labels and the predicted values. This can be done with stochastic gradient descent. 

\section{Evaluation Metrics}
Sometimes accuracy and error rates can be misleading. Consider a data set that is heavily biased towards one of the labels i.e. the majority of the labels are classified negative. A lazy majority classifier could outperform a classifier that received a substantial amount of training! Obviously, this is not ideal and we should find a better evaluation metric that takes into account how balanced a dataset is.

A useful tool is the confusion matrix. Consider the following dataset.
\begin{center}
    \begin{tabular} {| c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c |}
        \hline
        True label & - & - & - & - & - & + & - & - & - & - & - & + & - & - &  + & - & - \\
        \hline
        Predicted label & - & - & - & - & - & + & + & - & - & - & - & - & + & - &  + & - & - \\
        \hline
    \end{tabular}
\end{center}
\vspace{.5cm}
The confusion matrix is the following:
\begin{center}
    \begin{tabular}{| c | c | c |}
        & True Label Positive & True Label Negative \\
        \hline
        Predicted Label Positive & 2 (True Positive) & 2 (False Positive) \\
        \hline
        Predicted Label Negative & 1 (False Negative) & 16 (True Negative) \\
    \end{tabular}
\end{center}

\begin{gather*}
    \text{Accuracy} = \frac{TP + TN}{TP + TN + FN + FP} = \frac{18}{21} = .86 \\
    \text{Precision} = \frac{TP}{TP + FP} = \frac{2}{4} = .5 \\
    \text{Recall} = \frac{TP}{TP + FN} = \frac{2}{3} \approx .67 \\
\end{gather*}

We can see that both the precision and the recall are much lower than the accuracy due the imbalanced dataset.

We can define a new metric, the F1 score, which is just the harmonic mean of the precision and recall.
$$
\frac{1}{F_1} = (\frac{1}{P} + \frac{1}{R}) / 2 = \frac{2TP}{2TP + FP + FN}
$$