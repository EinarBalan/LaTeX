\chapter{Turing Machines}

As we showed in the last chapter with our discussion of Pumping Lemma, DFA's are not able to compute all functions. For example, it is impossible to compute if a string is a palindrome using a DFA. This is due to the limitation of memory and only single pass algorithms. Can we create a model that can computes on arbitrary length inputs and is not subject to these limitations? 

We will discuss Turing Machines, introduced by Alan Turing in 1936. This model of computation is as powerful as it gets. It will give us the ability to move the "head" both directions on input and read and write to a variable amount of memory. Essentially,

\[
    \text{Turing Machines} = \text{DFAs + left/right movement on input + read/write memory}
\]

\subsection*{Anatomy of a Turing Machine}
A turing machine consists of an infinitely long tape, on which the inputs are loaded into the first n spots. It can be thought of as an array. There are a finite number, k, states. The head can move left, right, or stay depending on the current state and the current symbol being read. 

\begin{center}
    \img{./img/turing-machine.png}
\end{center}

In each step of computation, we are in a state i and read the bit at the head of the tape. We can take several actions including
\begin{itemize}
    \item changing the state
    \item write something new at the head 
    \item move the head (left, right, or stay)
\end{itemize}

\begin{definition}
    Turing Machines

    \begin{itemize}
        \item k states
        \item $\Sigma \supseteq \{ 0, 1, \triangleright, \null \varnothing \}$ (there is a finite alphabet)
        \begin{itemize}
            \item $\triangleright$ denotes the start of the tape
            \item $\varnothing$ denotes nothing is at that position in the tape
        \end{itemize}
        \item Transition function $\delta: \{0, 1, ..., k - 1 \} \times \Sigma \times \{ L, R, S, H \}$
        \begin{itemize}
            \item $\delta(\text{state}_i, a) = (\text{state}_j, b, L)$
            \item a is the symbol being read, state$_j$ is the new state to go to, b is the symbol to write at the head, and L is the direction to go in
            \item H stands for halt
        \end{itemize}
    \end{itemize}
\end{definition}

Computation:
\begin{verbatim}
Start with head at x[0]
State: "0" (starting state)
Repeat:
    (new_state, new_symbol, A) = delta(current_state, Tape[Head])
    cur_state = new_state
    Tape[Head] = new_symbol
    if A == L: Head = max(0, Head - 1)
    if A == R: Head += 1
    if A == S: Head = Head
    if A == H: exit()
\end{verbatim}

By convention if a Turing Machine M halts on an input, then the output is the contents of the tape up to and including the head. If M does not halt, M(x) = "$\perp$". We say a function is computed by a TM M if f(x) = M(x) for all x. A language L is recognized by M if $x \in L \implies M(x) = 1$ and $x \notin L \implies M(x) = 0$.

\begin{example}
    
    $ k = 1, \Sigma = \{0, 1, \triangleright, \varnothing \}$
    \begin{gather*}
        \delta_M(0, 0) = (0, 1, R) \\
        \delta_M(0, 1) = (0, 0, R) \\ 
        \delta_M(0, \varnothing) = (0, \varnothing, H) \\
    \end{gather*}

    This Turing Machine simply computes the complement of the input. That is if we load in 1001, we will get 0110 as output. 

\end{example}

\begin{example}
    Now we show that Turing Machines are more powerful than DFA's by showing we can compute a function that DFA's cannot: MAJ.

    \begin{gather*}
        MAJ:\{0, 1\}^* \rightarrow \{0, 1\} \\
        MAJ(x) = 
        \begin{cases}
            1 & \text{if there are at least as many 1's as 0's} \\
            0 & \text{else}
        \end{cases}
    \end{gather*}
    
    The idea here is to try to match every 0 we see to a 1. If we are unable to match a 0, then there are more 0's than 1's and we output 0. 
    
    Pseudocode:
    \begin{verbatim}
    1. Scan to to the right until a 0 is found
    2. If no 0 is found:
        "clean up" the tape and return 1
    3. If a 0 is found:
        Mark the 0 as seen 
        Go to the start of the tape
        Scan the input to find a 1
        If 1 found:
            Mark 1 as seen
            Go to start
        If 1 not found:
            clean up and return 0
    \end{verbatim}
    
    Click \href{http://turingmachinesimulator.com/shared/ftcgvwwbaj}{here} for a visualization.   
\end{example}

\begin{example}
    Another function that DFA's could not compute was palindrome. Recall that palindrome returned 1 if the input string was the same forwards as it was backwards.

    Here the idea is to look at the first symbol in the string, then proceed to the end and look at the last symbol. If they are not the same, we return 0. If they are the same, we mark both symbols as seen and proceed to the first unseen symbol and repeat. 

    Pseudocode:
    \begin{verbatim}
    1. Enter state that remembers the first bit x: GoEndx and mark first bit as seen 
    (overwrite with a)
    2. Go to the end
    3. If the last bit matches x 
        Replace it with nothing and proceed to first unseen bit
        Repeat
    4. If the last bit does not match x
        Clean up the tape and return 0
    5. If there are no bits remaining that have not been seen:
        Clean up the tape and return 1
    \end{verbatim}

    I will omit the precise transition function definitions, but that along with a visualization can be found \href{http://turingmachinesimulator.com/shared/mngibtvnaj}{here}. 
\end{example}

From these two examples, we can see that Turing Machines are more powerful than DFA's as they can compute functions that DFA's cannot. We can take this a step further and say that Turing Machines are essentially as powerful as it gets. They are functionally equivalent to modern day programming languages and can simulate things such as random access.

\begin{theorem}
    
    For every python program P, $\exists$ a TM M such that $\forall$ x P(x) = M(x). In other words, Turing machines are computationally equivalent to Python. If P takes T time to compute, M will take $T^2$ time. 
\end{theorem}

\begin{theorem}

    A function f is computable if there exists a TM M for which f(x) = M(x) for all x. 
\end{theorem}

\vspace{1cm}

\begin{mdframed}[backgroundcolor=framebackground]
    \kw{Thesis}
    Church-Turing Thesis
    
    \vspace{.25cm}

    Every function that is computable by physical means is computable by a Turing Machine.
\end{mdframed}

\pagebreak

The fact that every python program (and in fact every implementation of a function) can be simulated using a Turing Machine buys us some very nice abilities. We call them, "Having Our Cake and Eating it Too."

\begin{definition}
    HOCAEIT Principle

    HOC: To show something is computable you can use a high level programming language. \\
    EIT: To show something is uncomputable, we only have to show that turing machines cannot compute it.
\end{definition}

\section{Universality}

Just like for circuits, we can create a universal turing machine that can simulate all turing machines. As before, we use the idea of code as data to implement this. If we can encode a Turing Machine as a binary string then we can pass them as input to a Turing Machine (which essentially compiles \& executes it). First, let us think about how we might encode any arbitrary Turing Machine.

\subsection*{Encoding}
Recall the definition of a Turing Machine, which can be completely described by a transition function $\delta: [k] \times \Sigma \rightarrow [k] \times \Sigma \times \{ L, R, S, H \}$, where k is the number of states and $\Sigma$ is the alphabet. We can represent every possible set of inputs and outputs to $\delta$ as a 5-tuple as in the following table:

\vspace{.25cm}
\begin{center}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \bf{State} & \bf{Input} & \bf{New State} & \bf{Write Bit} & \bf{Action} \\
        \hline
        0 & $a_0$ & 17 & $a_{10}$ & R \\
        \hline 
        0 & $a_1$ & 10 & $a_4$ & L \\
        \hline 
        ... & ... & ... & ... & ... \\
        \hline 
        k - 1 & $a_{l - 1}$ & 4 & $a_0$ & S \\
        \hline 
    \end{tabular}
\end{center}
\vspace{.25cm}

We use this fact to determine the following encoding form where l is the number of symbols in the alphabet and k is the number of states:

\[
    (k, l, ((0, a_0, 17, a_10, R), (0, a_1, 10, a_4, L), ..., (k - 1), a_{l-1}, 4, a_0, S))
\]

We then encode every integer as a binary string using $ZtoB$ and every tuple using some prefix free encoding. In total the length of the encoding will $O(kl(logk + logl))$. 
\begin{itemize}
    \item we have $kl$ tuples
    \item each tuple has 5 integers, which can be represented in $O(logk + logl)$ (since each integer is at most max($k, l$))
\end{itemize}

\begin{theorem}

    A turing machine M can be represented as a binary string of length $O(kl(logk + logl))$ denoted $\left<M\right>$. 
\end{theorem}

Note: This is a one-to-one relationship, so there are strings that are not valid turing machines. In this case, we pair these strings with the trivial TM that always outputs 0. 

\pagebreak

\subsection*{Evaluation}
Now that we have successfully encoded our TM, we want to define a function that can determine the output of any encoded TM passed into it.

\begin{definition}
    $EVAL: \{0, 1\}^* \rightarrow \{0, 1\}^* \text{ }\cup \perp$

    \begin{equation*}
        EVAL(M, x) = \begin{cases}
            M(x) & \text{if M is a valid TM} \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
\end{definition}

\theoremproof{
    
    EVAL is computable (Turing 1936). There exists a TM U such that $U(M, x) = EVAL(M, x)$ $\forall$ inputs.
}
{
    
    We can use the HOC principle to show that EVAL is computable by a TM by showing it is computable with a Python program. The specific program is as follows (notice it is very similar to the psuedocode we used to define Turing Machines)

    \begin{center}\img{./img/tm-python.png}\end{center}

    We know that since every program has a corresponding TM, there must be a TM to compute EVAL.
}

\subsection*{Implications of Universality}

A universal TM is essentially just a general purpose computer. Any function can be computed using it. We have relatively small universal TM's using just 25 states and the standard alphabet. This implies that we can compleley describe a model to compute any function using just 500 numbers (25 states x 4 symbols x 5-tuple).

Some more notes:
\begin{itemize}
    \item this brings up the concept of metacircular evaluators; for example the C compiler is written in C 
    \item universality transcends the specific model  
    \begin{itemize}
        \item there exists a python program that runs all python programs
        \item there exists a python program that compiles and executes all java programs
    \end{itemize}
\end{itemize}

\begin{definition}
    Turing Complete Languages

    Anything that can be used to simulate a universal turing machine is considered turing complete. The implications of this are quite interesting$:$ any turing complete language can be used to implement any programming language or compute any function that is computable. For example, theoretically we can use \LaTeX{} to write a C compiler or HTML+CSS to write a Python interpreter (both of these are Turing Complete)! 
    
    Check \href{https://en.wikipedia.org/wiki/Turing_completeness#Examples}{here} for more examples of Turing Complete languages (including some very unexpected ones like Minecraft).
\end{definition}

\section{Uncomputability}
The statement "A function is computable if there is a Turing Machine that computes it" implies the existence of uncomputable functions and indeed this is true.

\begin{theorem}
    There exist uncomputable functions.
    
    A function f is uncomputable if it cannot be computed by a TM.
\end{theorem}

Consider a new function, the "Toddler" function, that we define below:

\begin{gather*}
    \text{Todd}: \{0, 1\}^* \rightarrow \{0, 1\} \\
    \text{Todd}(\left<M\right>) = \begin{cases}
        1 & \text{if $M(\left<M\right>)$ halts and the first bit of output is 0} \\
        0 & \text{otherwise}
    \end{cases}
\end{gather*}

In other words, TODD does the opposite of what M does, much like a toddler. We will see that TODD is uncomputable. Another way to think of TODD is as follows:

\begin{gather*}
    \text{Todd}: \{0, 1\}^* \rightarrow \{0, 1\} \\
    \text{Todd}(\left<M\right>) = \begin{cases}
        1 & \text{if $M(\left<M\right>)$ halts and the first bit of output is 0} \\
        0 & \text{if $M(\left<M\right>)$ halts and the first bit of output is 1} \\
        0 & \text{if $M(\left<M\right>)$ does not halt} \\
    \end{cases}
\end{gather*}

\begin{proof}
    
    Suppose there was a machine, N, that computed Toddler. Consider what should happen if we try to compute TODD using N given the description of N as input. That is, what is $N(\left<N\right>)$? It should be equal to Todd($\left<N\right>$) if N computes Todd. However, we will see that by definition this is not the case. 

    Case 1: If $N$ halts on $\left<N\right>$ and the output is 1
    \begin{itemize}
        \item then Todd($\left<N\right>$) should be 0
        \item clearly N does not compute Todd since the outputs are not the same
        \item we have reached a contradiction
    \end{itemize}
    Case 2: If $N(\left<N\right>)$ halts and the output is 0
    \begin{itemize}
        \item then Todd($\left<N\right>$) should be 1
        \item once again, N does not compute Todd since the output differs
        \item we have reached a contradiction
    \end{itemize}
    Case 3: If If $N(\left<N\right>)$ does not halt
    \begin{itemize}
        \item we will never know what to output 
        \item Toddler is nicely defined in this case (it should equal 0)
        \item N cannot compute Toddler since it does not have the same output as Toddler would have in this case
    \end{itemize}

    In every case, we see that $N$ returns the wrong value so it cannot be that it computes Todd. (Reminder that a TM $M$ is said to compute a function $f$ when for all inputs $x$, $M(x) = f(x)$). So we conclude by contradiction that Todd is uncomputable.
    
    \qed

    \hr

    An alternative vizualization once again starts with the assumption that $N$ computes TODD. If this is true then following must hold:
    \begin{gather*}
        N(\left<N\right>) = \text{TODD}(\left<N\right>) = \begin{cases}
            1 & \text{if $N(\left<N\right>)$ halts and first bit is 0} \\
            0 & \text{otherwise}
        \end{cases}
    \end{gather*}
    But this is clearly a contradiction because it implies that $N(\left<N\right>) = \lnot N(\left<N\right>)$. It's like saying 0 = 1, which is clearly not true. So TODD must be uncomputable.

    \qed
\end{proof}

Intuitively, the main issue with the machine N that "computes" Toddler is that N might not halt. The fact that Toddler is uncomputable implies that many other more practical problems are uncomputable. Consider this: can we create a program to determine if another program will terminate? This would be very useful in applications such as OS or designing programming languages or debuggers, etc. 

We can define HALT as follows:
\begin{gather*}
    \text{HALT}: \{0, 1\}^* \rightarrow \{0, 1\} \\
    \text{HALT}(\left<M\right>, x) = \begin{cases}
        1 & \text{if $M$ halts when run on $x$} \\
        0 & \text{else}
    \end{cases}
\end{gather*}

Unfortunately, halt is uncomputable.

\begin{proof}
    
    We can use a concept known as reduction to show this. If HALT was computable, then we could use it to compute TODD as follows.

    Solve Todd using Halt:
    \begin{verbatim}
    1. If Halt(<M>, <M>) = 1
        Run M on <M>
        Output the opposite of the first bit.
    2. Else
        Output 0
    \end{verbatim}

    If you're paying close attention, an important question arises: why doesn't the same contradiction from before hold when using HALT to compute TODD? Specifically, why is it that we can safely determine the output of <M> and return the opposite in a way that does not contradict the original function? This is mostly because we are not using N (the turing machine assumed to compute TODD) to directly compute TODD. We first check if N halts, and then output the opposite of its result if so. The problem with using N directly was that the machine could end up in a loop, and in this case we don't know what the opposite of our output was (there will never be any). Instead, we are using a different Turing Machine that uses HALT as a blackbox in order to, via its own definition, compute TODD. So if Halt was computable, then it would also be possible to compute TODD (and $x$ would equal $\lnot x$). However, clearly this cannot be the case since we proved TODD was uncomputable. It must be that HALT is uncomputable as well. 
    
    \qed    

\end{proof}

Is Halt really a difficult problem? The short answer is yes, even for a relatively simple programs. Consider how you might write a program that determines if the Goldback Conjecture was true. The Goldbach Conjecture says that every even number can be written as a sum of two primes.

We can write a program as follows:

\begin{verbatim}
def isPrime(n):
    for a in range(2, n):
        if n % a == 0:
            return 0
    return 1
def GOLDBACH():
    n = 4
    while True:
        goldbachN = false
        for p in range(2, n):
            if isPrime(p) and isPrime(n - p):
                goldbachN = True
                break
        if goldbachN: n += 2
        else: return False
\end{verbatim}

It is difficult to tell if this program will halt just by looking at it. It will only halt if the Goldbach conjecture is false, otherwise it will proceed forever (since there are an infinite number of even numbers to consider). So yeah. Halt is a hard problem. If HALT was computable, we could use it to determine the answers to many other problems (such as the Goldbach conjecture, which to this day is unproven).

\section{Reduction}
Put simply, a reduction from A to B means that we use B to solve A.

We showed before how to solve Toddler using Halt. This proved to us that Halt was uncomputable since we already knew Toddler was uncomputable. We can generalize this same approach to work for other problems. If we have two problems: A and B. We know that B is uncomputable. If we can reduce problem A to problem B, then this is proof that B is also uncomputable.

This works because if we can use problem B to solve problem A, then it must be that A is computable if B is computable. However, if we know that A is uncomputable then we know that B is uncomputable as well. This is because the second statement is the contrapositive of the first. If we could solve B, then we must also be able to solve A. This is a contradiction since we know A is uncomputable.

\subsection*{Turing/Cook Reductions}
We can use the blackbox for B multiple times. For example, if $f$ is uncomputable then it must be that $NOTf$ is uncomputable as well. We can prove this by contradiction. If $NOTf$ was computable, we could use it to solve $f$ by taking its opposite. However, this is a contradiction because we know $f$ is uncomputable. Therefore, $NOTf$ must uncomputable as well.

\subsection*{KARP Reductions}
In KARP reductions, we reduce problem A to problem B by transforming the inputs for A to inputs for B. Specifically, we use function $R: \{\text{inputs for A}\} \rightarrow \{ \text{inputs for B}\}$.

There are three parts for KARP Reduction.
\begin{enumerate}
    \item Specification (What): $R$
    \item Utility (Why): $\forall x A(x) = B(R(x))$
    \item Implementation (How): Need to make sure R is computable
\end{enumerate}

Consider a new function:
\begin{gather*}
    \text{HALTONZERO}: \{0,1\}^* \rightarrow \{0,1\} \\
    \text{HALTONZERO}(\left<M\right>) = \begin{cases}
        1 & \text{if M halts on input 0} \\
        0 & \text{if M does not halt on input 0} \\
    \end{cases}
\end{gather*}

We can create a reduction function that maps inputs (M, x) to a turing machine described by the following pseudocode:
\begin{verbatim}
def N(z):
    res = EVAL(m, x)
    return res
\end{verbatim}
This reduction function $R$ returns a turing machine that computes the output of the turing machine that is given to it as input.

\pagebreak
We can easily show that $\forall x \text{HALT}(M, x) = \text{HALTONZERO}(R(M,x))$. Note that the input to TM N is ignored.
\begin{itemize}
    \item If M halts on x, then N also halts on all inputs (including 0) so HALT(M, x) = HALTONZERO(N) = 1
    \item If M does not halt on x, then N does not halt on all inputs (including 0) so HALT(M, x) = HALTONZERO(N) = 0
\end{itemize}

We have succesfully shown the reduction holds. So since, HALT is uncomputable we know HALTONZERO must also be uncomputable.