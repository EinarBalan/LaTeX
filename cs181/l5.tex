\chapter{Non-Deterministic Finite Automata}

To begin discussion of NFA's, we will first discuss function concatenations. For convenience, the definition is repeated below:

\begin{definition}
    Concatenation

    f, g are functions from binary strings to binary. The concatenation of f, g $f \circ g = 1 \iff f(x_1) = 1 \text{ and } g(x_2) = 1$ where $x_1, x_2$ are consecutive substrings of x.
\end{definition}

\begin{example}
    Function Concatenation

    Consider $f_1$ and $f_2$. $f_1$ returns 1 for all x and $f_2$ returns 1 if x starts with 1 and has length exactly 4. Both of these are computable by DFA's as shown below (the constant one is pretty obvious so it has been omitted). 

    \begin{center}
        \img{./img/dfa-6.png}
    \end{center}

    Can we compute the concatenation of these two functions using a DFA? In fact, we can. Using just 16 states, we can use a DFA to compute it. To clarify what exactly we're computing, $f_1 \circ f_2(001010) = 1$ since $f_1(00) = 1$ and $f_2(1010) = 1$.
\end{example}

\begin{example}
    
    Consider a function $f_{reverse}(x)$, which simply returns the value of f when input with the reverse of x i.e. the bits are written right to left. Can we compute this with a DFA? 

    It seems to be exactly the opposite of what we can compute using a DFA. DFA's always implement one pass, constant memory algorithms and it would seem necessary to violate this in order to implement $f_reverse$. We will revisit this subject. 
\end{example}

\subsection*{Non-Deterministic Finite Automata}
\begin{center}
    \img{./img/nfa-1.png}
\end{center}

NFA's are similar to DFA's, with some small variations:
\begin{itemize}
    \item they can have mulitple outgoing edges with the same states
    \item some edges can be missing (by convention, this indicates they lead to dead states)
    \item some edges are labeled $\varepsilon$
\end{itemize}

\begin{example}
    \begin{center}
        \img{./img/nfa-2.png}
    \end{center}

    The above represents the branching diagram of the NFA. If any branch has a state within S after the last bit is read, then 1 is output.
\end{example}

\begin{definition}
    Non-Deterministic Finite Automata

    An NFA, N is defined by a transition function and a set of accepting states. So N = (T, S).

    \begin{gather}
        T: [C] \times \{0, 1, \varepsilon\} \rightarrow \text{Power}([c]) \\
        S \subseteq [c] \\
        Power([c]) = \{I: I \subseteq [c] \}
    \end{gather}

    In other words, the transition function is mapping the current state and the current bit input to a subset of all states in the NFA.
\end{definition}

\subsection*{Why NFA's}
One might wonder, what are NFA's good for? Well, we can use them to compute the concatenation from earlier where $f_1$ is a constant and $f_2$ is 1 if the first bit is 0 and the input length is exactly 4. Logically, the concatenation of these two outputs 1 if the 4th bit from the end is 1 and 0 otherwise. The NFA is pictured below:
\begin{center}
    \img{./img/nfa-3.png}
\end{center}

Important Takeaway: In general, if two functions are computable by DFA's, we can compute their concatentation using an NFA in which we simply add $varepsilon$ transitions pointing from accepting states of the first function to the start of the second.
\begin{center}
    \img{./img/nfa-4.png}
\end{center}

\subsection*{Kleen* Operation on Functions}
We already discussed Kleen* as a set operation, but as a function operation it is defined as f*(x) = 1 if x can be broken up into $x_0, x_1, ..., x_n$ such that $f(x_i) = 1$ for all i.

Building upon the takeaway from earlier, if f is computable by a DFA, then f* is computable by an NFA. We simply add a dummy state leading into the previous DFA's initial state and add epsilon transitions pointing from all accepting states to the initial state.
\begin{center}
    \img{./img/nfa-5.png}
\end{center}

\begin{example}
    Construct an NFA where L = \{ all strings that have a 1 in the last three positions \}.

    \begin{center}
        \img{./img/nfa-6.png}
    \end{center}

    This works by branching off at bit and checking 1) if there are less than or equal to three bits remaining and 2) if one of those bits is a 1 (we only progress to an accepting state if 1 is seen and we go to dead state if more than three bits are read).
\end{example}

\hr 

Recall the $f_{\text{reverse}}$ example. $f_{\text{reverse}}(x)$ is one if $f$ is 1 when the input string is the reverse of x. If $f$ is computable by a DFA, what about $f_{\text{reverse}}$? 

We can compute it with an NFA! The process is as follows:
\begin{enumerate}
    \item add a new start state to the DFA for f
    \item add $\varepsilon$ transitions from new starting state to all previous accepting states
    \item reverse the direction of all arrows
    \item make old start state the new accept state
\end{enumerate}

\section{NFA's vs DFA's: Computability}
One might wonder, are NFA's more powerful than DFA's? They seem to have more functionality with epsilon transitions and multiple possible outgoing edges from the same input bit, so one would expect that this is the case. However, a \kw{mind boggling} theorem shows that this is not the case! 

\theoremproof{
    Every NFA has an equivalent DFA!

    For every NFA N, $\exists$ a DFA D such that N(x) = D(x) $\forall$ x. As a result, the concatenation of two functions, the kleene star operation on a function, and 
}
{
    The main idea is that for each level of our NFA input tree we need to know what states are reachab le at that level. We can merge redundant states at the same level into one. 

    First, we will assume the case where there are no $\varepsilon$ transitions. Given an NFA $N = (T_N, S_N)$The goal is to find a DFA $D = (T_D, S_D)$ such that $N(x) = D(x) \forall x \in \{0,1\}^*$. If the NFA is on states [c], we will construct a DFA whose states corresond to subsets of Power([c]) i.e. if c = 3, the DFA states will be $\emptyset, \{0\}, \{1\}, \{2\}, \{0, 1\}, \{1, 2\}, \{0, 2\}, \{0, 1, 2\}$. The number of states in our new DFA is $2^c$.

    We construct $T_D$ with the following: 
    $$ T_D: Power([c]) \times \{0,1\} \rightarrow Power([c])$$
    $$ T_D(I, a) = \bigcup_{i \in I}T_N(i, a) $$

    And the accepting states:
    $$ S_D = \{ R \subseteq [c]: R \cap S_T \neq \emptyset $$

    The start state should just be \{0\}.

    In words for each of these, for each element in I we group together the results from $T_N$ removing duplicates. For $S_D$, we include all subsets that have a value in the original set of accepting states.

    \hr 

    What if we add epsilon transitions?

    \begin{definition}
        Eps(I) = all states reachable by taking $varepsilon$ edges from i + \{i\}
    \end{definition}

    To finish, we simply wrap our transition function and new start state with Eps and we are done.

    $$ T_D(I, a) = Eps(\bigcup_{i \in I}T_N(i, a)) $$
    The start state should just be Eps(\{0\}).
    \qed
}

\begin{example}
    Given an NFA,
    \begin{center}
        \img{./img/nfa-7.png}
    \end{center}

    We can construct a DFA like the following (incomplete diagram),
    \begin{center}
        \img{./img/dfa-7.png}
    \end{center}

    Note that the $\emptyset$ state denotes the dead state. We create a state for each subset of states in the NFA and use the formula above to get arrows. The accepting states are simply all sets that contain the accepting states from the
\end{example}

\section{Pattern Matching}
In a standard pattern matching problem, we have some 
"text" and a "pattern," with the length of the pattern being much shorter than the length of the text. The question is, does the pattern occur within the text?

A naive O(m * n) algorithm is the following (assuming p is lengthm and x is length n):
\begin{verbatim}
def patternmatch(x, p):
    l = len(p)
    for i in range(0, len(x) - l):
    if x[i, i + l] == p:
        return 1
    return 0
\end{verbatim}

We can do better. Using the Knuth-Morris-Pratt (KMP) algorithm, we get an O(m + n) single-pass algorithm
\begin{itemize}
    \item given P, first find a DFA D on m + 1 states O(m)
    \item for any string x, p occurs in x $\iff$ D(x) = 1
    \item now mimic behaviour of D on x O(n)
\end{itemize}

\hr

\subsection*{Regular Expressions}
Regular expressions are a programming tool used to efficiently find some pattern within some text. We will formally define them here.

\begin{definition}
    
    Base cases:
    \begin{itemize}
        \item "0" is a regex
        \item "1" is a regex
        \item $\emptyset$ is a regex
        \item $varepsilon$ is a regex (empty string)
    \end{itemize}

    Compound cases ($r_1, r_2$ are regex):
    \begin{itemize}
        \item $r_1r_2$ (concatenation) is a valid regex
        \item $r_1^*$ is a valid regex (repetition 0-n times)
        \item $(r_1 | r_2)$ is a valid regex (r1 or r2)
    \end{itemize}
\end{definition}

\begin{example}
    \begin{enumerate}
        \item Does x = 0 match (0 | 1)? yes
        \item x = 01 match (0 | 1)(0 | 1)? yes
        \item x = 00 match (0 | 1)1(0 | 1)? no
        \item What matches $(0 | 1)^*$? All strings
        \item $(0 | 1)^*0$? strings ending in 0
        \item $(10)^*$? repetitions of 10
        \item $(0(10)^* | (10)^*1 | (01)^* | (10)^*)$? all strings with alternating symbols
        \item $0^*10^*10^*10^*$ strings with exactly three 1's
        \item $((0^*10^*10^*10^*)^* | 0^*)$? all strings with number of ones divisible by 3
        \item $(0 | 1)^* 1 (0|1)(0|1)(0|1)$? all strings with 1 4th bit from the end
    \end{enumerate}
\end{example}

\begin{theorem}
    Regex is computationally equivalent to a DFA.

    For every regex r, there is a DFA D such that the output of the regex is the same as the output of the DFA for all x. 

    For every DFA D, there is a regex r such that the output of the regex is the same as the output of the DFA for all x. A function is considered \kw{regular} if there is a regex (or DFA) computing it.
\end{theorem}

We can compute regex pattern matching in O(n * m) time. The GREP algorithm is the following:
\begin{itemize}
    \item convert the regex to an equivelant NFA N with O(m) states and transitions in O(m) time
    \item simulate NFA N on the input X in O(m * n) time
\end{itemize}

\begin{proof}
    By construction,

    We defined regex inductively, so we will alos build the NFA for a regex inductively.

    Base cases:
    \begin{itemize}
        \item "0" 
        \begin{itemize}
            \item \img{./img/regex-nfa-1.png}
        \end{itemize}
        \item "1"
        \begin{itemize}
            \item \img{./img/regex-nfa-2.png}
        \end{itemize}
        \item "$\varepsilon$" (empty string)
        \begin{itemize}
            \item \img{./img/regex-nfa-3.png}
        \end{itemize}
    \end{itemize}

    Compound cases:
    \begin{itemize}
        \item concatenation i.e. r = $r_1r_2$
        \begin{itemize}
            \item we saw that we can compute the concatenation of two NFA's by adding epsilon transitions from the accepting state of the first to the initial state of the second
        \end{itemize}
        \item union i.e. r = $r_1 | r_2$
        \begin{itemize}
            \item we can compute or of two NFA's by adding an epsilon transition from the start of one to the start of the other
        \end{itemize}
        \item kleen* i.e. r = $r_1*$
        \begin{itemize}
            \item we saw that we can compute kleen* by adding a new (accepting) start state and epsilon transitions from accepting states to initial starting state
        \end{itemize}
    \end{itemize}

    We have shown that there is an equivalent NFA for every regex. A regex can be converted into an NFA in linear time (as done in GREP).
\end{proof}

\hr 

How can an NFA be simulated efficiently? If an NFA has m states, we can simulate it on a string of length n in $O(nm^2)$ time by only keeping track of one copy for each reachable branch.

\hr 

\section{Can DFA's compute everything?}

We know that DFA's are good for algorithms that require only constant memory and a single pass. Does this suffice for any function? Consider the following:

\begin{equation*}
    MAJ(x) = 
    \begin{cases}
        1 & \text{if at least half the input bits are 1} \\
        0 & \text{else}
    \end{cases}
\end{equation*}

MAJ is not regular, meaning there is no DFA that can compute MAJ. This is because we have to "count" the nuber of bits that are 1. 

\begin{example}
    
    $L_1 = \{ x: \text{contains an equal number of 0's and 1's}\} $

    This is also not computable by DFA's for the same reason.
\end{example}

Based on these two examples, one might thing that anytime "counting" is required for a function that it is not regular. However, this is not necessarily the case. In some cases, there might be shortcuts that yield the same result using a DFA. Consider the following

\[
    L_2 = \{x: \text{contains an equal number of (01)'s and (10)'s}\}
\]

$L_2$ is actually regular! In this case, there are an equal number of 01 and 10 if and only if the start and end bits are the same. So we can easily create a DFA for this. 

In order to prove functions are not regular, we make use of the "Pumping Lemma."

\subsection*{Pumping Lemma}
If f is a regular function, there exists a number p such that every string x where f(x) = 1 of length $\ge$ p, can be written as x = $a \circ b \circ c$.
\begin{itemize}
    \item $f(a \circ b^i \circ c) = 1$ for all $i \ge 0$
    \item length of b > 0
    \item length of $a \circ b \le p$
\end{itemize}

Intuitive description: If a language L is regular then every sufficiently long string $x \in L$ has a piece that can be repeated an arbitrary number of times while still being in L.

We can use this to prove that in the example above, MAJ is not regular.

\begin{proof}

    Towards a contradiction, suppose MAJ is regular. This implies there must exist a number P such that the conditions of pumping lemma hold.

    Consider the string x that has p 0's followed by p 1's. By pumping lemma, there should be a way to split x into three pieces such that the middle piece can be repeated an arbitrary number of times and the properties of pumping lemma still hold. However this is not the case due to the condition that the length of ab must be less than p. 

    Due to this restriction in length, the split between a and b will always be inside the 0's, meaning b will consist only of 0's. We cannot repeat b an arbitrary number of times because this will cause the output of the function to change to 0. This is a contradiction. Therefore, MAJ is not regular.
    
    \qed
\end{proof}