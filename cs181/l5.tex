\chapter{Non-Deterministic Finite Automata}

To begin discussion of NFA's, we will first discuss function concatenations. For convenience, the definition is repeated below:

\begin{definition}
    Concatenation

    f, g are functions from binary strings to binary. The concatenation of f, g $f \circ g = 1 \iff f(x_1) = 1 \text{ and } g(x_2) = 1$ where $x_1, x_2$ are consecutive substrings of x.
\end{definition}

\begin{example}
    Function Concatenation

    Consider $f_1$ and $f_2$. $f_1$ returns 1 for all x and $f_2$ returns 1 if x starts with 1 and has length exactly 4. Both of these are computable by DFA's as shown below (the constant one is pretty obvious so it has been omitted). 

    \begin{center}
        \img{./img/dfa-6.png}
    \end{center}

    Can we compute the concatenation of these two functions using a DFA? In fact, we can. Using just 16 states, we can use a DFA to compute it. To clarify what exactly we're computing, $f_1 \circ f_2(001010) = 1$ since $f_1(00) = 1$ and $f_2(1010) = 1$.
\end{example}

\begin{example}
    
    Consider a function $f_{reverse}(x)$, which simply returns the value of f when input with the reverse of x i.e. the bits are written right to left. Can we compute this with a DFA? 

    It seems to be exactly the opposite of what we can compute using a DFA. DFA's always implement one pass, constant memory algorithms and it would seem necessary to violate this in order to implement $f_reverse$. We will revisit this subject. 
\end{example}

\subsection*{Non-Deterministic Finite Automata}
\begin{center}
    \img{./img/nfa-1.png}
\end{center}

NFA's are similar to DFA's, with some small variations:
\begin{itemize}
    \item they can have mulitple outgoing edges with the same states
    \item some edges can be missing (by convention, this indicates they lead to dead states)
    \item some edges are labeled $\varepsilon$
\end{itemize}

\begin{example}
    \begin{center}
        \img{./img/nfa-2.png}
    \end{center}

    The above represents the branching diagram of the NFA. If any branch has a state within S after the last bit is read, then 1 is output.
\end{example}

\begin{definition}
    Non-Deterministic Finite Automata

    An NFA, N is defined by a transition function and a set of accepting states. So N = (T, S).

    \begin{gather}
        T: [C] \times \{0, 1, \varepsilon\} \rightarrow \text{Power}([c]) \\
        S \subseteq [c] \\
        Power([c]) = \{I: I \subseteq [c] \}
    \end{gather}

    In other words, the transition function is mapping the current state and the current bit input to a subset of all states in the NFA.
\end{definition}

\subsection*{Why NFA's}
One might wonder, what are NFA's good for? Well, we can use them to compute the concatenation from earlier where $f_1$ is a constant and $f_2$ is 1 if the first bit is 0 and the input length is exactly 4. Logically, the concatenation of these two outputs 1 if the 4th bit from the end is 1 and 0 otherwise. The NFA is pictured below:
\begin{center}
    \img{./img/nfa-3.png}
\end{center}

Important Takeaway: In general, if two functions are computable by DFA's, we can compute their concatentation using an NFA in which we simply add $varepsilon$ transitions pointing from accepting states of the first function to the start of the second.
\begin{center}
    \img{./img/nfa-4.png}
\end{center}

\subsection*{Kleen* Operation on Functions}
We already discussed Kleen* as a set operation, but as a function operation it is defined as f*(x) = 1 if x can be broken up into $x_0, x_1, ..., x_n$ such that $f(x_i) = 1$ for all i.

Building upon the takeaway from earlier, if f is computable by a DFA, then f* is computable by an NFA. We simply add a dummy state leading into the previous DFA's initial state and add epsilon transitions pointing from all accepting states to the initial state.
\begin{center}
    \img{./img/nfa-5.png}
\end{center}

\begin{example}
    Construct an NFA where L = \{ all strings that have a 1 in the last three positions \}.

    \begin{center}
        \img{./img/nfa-6.png}
    \end{center}

    This works by branching off at bit and checking 1) if there are less than or equal to three bits remaining and 2) if one of those bits is a 1 (we only progress to an accepting state if 1 is seen and we go to dead state if more than three bits are read).
\end{example}

\hr 

Recall the $f_{\text{reverse}}$ example. $f_{\text{reverse}}(x)$ is one if $f$ is 1 when the input string is the reverse of x. If $f$ is computable by a DFA, what about $f_{\text{reverse}}$? 

We can compute it with an NFA! The process is as follows:
\begin{enumerate}
    \item add a new start state to the DFA for f
    \item add $\varepsilon$ transitions from new starting state to all previous accepting states
    \item reverse the direction of all arrows
    \item make old start state the new accept state
\end{enumerate}

\section{NFA's vs DFA's: Computability}
One might wonder, are NFA's more powerful than DFA's? They seem to have more functionality with epsilon transitions and multiple possible outgoing edges from the same input bit, so one would expect that this is the case. However, a \kw{mind boggling} theorem shows that this is not the case! 

\theoremproof{
    Every NFA has an equivalent DFA!

    For every NFA N, $\exists$ a DFA D such that N(x) = D(x) $\forall$ x. As a result, the concatenation of two functions, the kleene star operation on a function, and 
}
{
    The main idea is that for each level of our NFA input tree we need to know what states are reachab le at that level. We can merge redundant states at the same level into one. 

    First, we will assume the case where there are no $\varepsilon$ transitions. Given an NFA $N = (T_N, S_N)$The goal is to find a DFA $D = (T_D, S_D)$ such that $N(x) = D(x) \forall x \in \{0,1\}^*$. If the NFA is on states [c], we will construct a DFA whose states corresond to subsets of Power([c]) i.e. if c = 3, the DFA states will be $\emptyset, \{0\}, \{1\}, \{2\}, \{0, 1\}, \{1, 2\}, \{0, 2\}, \{0, 1, 2\}$. The number of states in our new DFA is $2^c$.

    We construct $T_D$ with the following: 
    $$ T_D: Power([c]) \times \{0,1\} \rightarrow Power([c])$$
    $$ T_D(I, a) = \bigcup_{i \in I}T_N(i, a) $$

    And the accepting states:
    $$ S_D = \{ R \subseteq [c]: R \cap S_T \neq \emptyset $$

    The start state should just be \{0\}.

    In words for each of these, for each element in I we group together the results from $T_N$ removing duplicates. For $S_D$, we include all subsets that have a value in the original set of accepting states.

    \hr 

    What if we add epsilon transitions?

    \begin{definition}
        Eps(I) = all states reachable by taking $varepsilon$ edges from i + \{i\}
    \end{definition}

    To finish, we simply wrap our transition function and new start state with Eps and we are done.

    $$ T_D(I, a) = Eps(\bigcup_{i \in I}T_N(i, a)) $$
    The start state should just be Eps(\{0\}).
    \qed
}

\begin{example}
    Given an NFA,
    \begin{center}
        \img{./img/nfa-7.png}
    \end{center}

    We can construct a DFA like the following (incomplete diagram),
    \begin{center}
        \img{./img/dfa-7.png}
    \end{center}

    Note that the $\emptyset$ state denotes the dead state. We create a state for each subset of states in the NFA and use the formula above to get arrows. The accepting states are simply all sets that contain the accepting states from the
\end{example}

\section{Pattern Matching}
In a standard pattern matching problem, we have some 
"text" and a "pattern," with the length of the pattern being much shorter than the length of the text. The question is, does the pattern occur within the text?

A naive O(m * n) algorithm is the following (assuming p is lengthm and x is length n):
\begin{verbatim}
def patternmatch(x, p):
    l = len(p)
    for i in range(0, len(x) - l):
    if x[i, i + l] == p:
        return 1
    return 0
\end{verbatim}

We can do better. Using the Knuth-Morris-Pratt (KMP) algorithm, we get an O(m + n) single-pass algorithm
\begin{itemize}
    \item given P, first find a DFA D on m + 1 states O(m)
    \item for any string x, p occurs in x $\iff$ D(x) = 1
    \item now mimic behaviour of D on x O(n)
\end{itemize}

\hr

\subsection*{Regular Expressions}
Regular expressions are a programming tool used to efficiently find some pattern within some text. We will formally define them here.

\begin{definition}
    
    Base cases:
    \begin{itemize}
        \item "0" is a regex
        \item "1" is a regex
        \item $\emptyset$ is a regex
        \item $varepsilon$ is a regex (empty string)
    \end{itemize}

    Compound cases ($r_1, r_2$ are regex):
    \begin{itemize}
        \item $r_1r_2$ (concatenation) is a valid regex
        \item $r_1^*$ is a valid regex (repetition 0-n times)
        \item $(r_1 | r_2)$ is a valid regex (r1 or r2)
    \end{itemize}
\end{definition}

\begin{example}
    \begin{enumerate}
        \item Does x = 0 match (0 | 1)? yes
        \item x = 01 match (0 | 1)(0 | 1)? yes
        \item x = 00 match (0 | 1)1(0 | 1)? no
        \item What matches $(0 | 1)^*$? All strings
        \item $(0 | 1)^*0$? strings ending in 0
        \item $(10)^*$? repetitions of 10
        \item $(0(10)^* | (10)^*1 | (01)^* | (10)^*)$? all strings with alternating symbols
        \item $0^*10^*10^*10^*$ strings with exactly three 1's
        \item $((0^*10^*10^*10^*)^* | 0^*)$? all strings with number of ones divisible by 3
        \item $(0 | 1)^* 1 (0|1)(0|1)(0|1)$? all strings with 1 4th bit from the end
    \end{enumerate}
\end{example}

\begin{theorem}
    Regex is computationally equivalent to a DFA.

    For every regex r, there is a DFA D such that the output of the regex is the same as the output of the DFA for all x. 

    For every DFA D, there is a regex r such that the output of the regex is the same as the output of the DFA for all x.
\end{theorem}

We can compute regex pattern matching in O(n * m) time. The GREP algorithm is the following:
\begin{itemize}
    \item conver the regex to an equivelant NFA N with O(m) states and transitions in O(m) time
    \item simulate NFA N on the input X in O(m * n) time
\end{itemize}