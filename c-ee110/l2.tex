\chapter{Random Variables}
A \kw{random variable} is a variable whose specific value cannot be predicted with certainty before an experiment. They take on a numerical value for each possible event in the sample space.

\begin{example}
    Random variables are easy to define. For example,
    \begin{itemize}
        \item X = magnitude of a future earthquake
        \item Y = yield stress of a material
        \item Z = peak wind pressure during a given year
    \end{itemize}
\end{example}

For a random variable $X$, its outcomes are denoted $x_1, x_2, \ldots, x_n$. For an outcome $x_i$, we denote the probability of that outcome as $P(X = x_i)$.

\section{Discrete Random Variables}
A random variable is called \kw{discrete} if the number of outcomes is countable. For example, for X = the number of cars on a bridge at a certain time, X is discrete.

Distributions of discrete random variables can be quantified in 2 ways.
\begin{enumerate}
    \item Probability Mass Function
    \[
        p_X(x_i) = P(X = x_i)
    \]
    \begin{center}
        \img{./img/PMF.png}
    \end{center}

    \item Cumulative Distribution Function
    \[
        F_X(x_i) = P(X \le x_i)
    \]
    \begin{center}
        \img{./img/CDF.png}
    \end{center}
\end{enumerate}

Intuitively, adding up $p_X(x_i)$ for all i is equal to $F_x(a)$.
\[
    F_X(a) = \sum_{\text{all } x_i \le a} p_X(x_i)
\]

\subsubsection*{Rules of Discrete Random Variables}
\begin{itemize}
    \item $0 \le p_X(x_i) \le 1$
    \item $\sum_{\text{all } x_i} p_X(x_i) = 1$
    \item $F_X(-\infty) = 0$
    \item $F_X(+\infty) = 1$
    \item $F_X(b) \ge F_X(a) \text{ if } b \ge a$
\end{itemize}
All of these rules are fairly intuitive. For example, the probability of any event must be between 0 and 1. Additionally, the sum of all events in a sample space must be 1. 

\section{Continuous Random Variables}
Random variables are said to be \kw{continuous} if they can take on any real value. As a result, there are $\infty$ possible values for a random variable X. It follows that

\[
    P(X = x_i) = \frac{1}{\infty} = 0 \text{, for all } i
\]

We can describe the distribution of continuous random variables in 2 ways.
\begin{enumerate}
    \item Probability Density Function
    \[ 
        f_X(x_i)dx = P(x_i < X < x_i + dx)
    \]
    \begin{center}
        \img{./img/PDF.png}
    \end{center}

    We know that occurences in different intervals are mutually exclusive, so it follows that
    \[
        P(a < X \le b) = \int_a^bf_X(x)dx
    \]
    \item Cumulative Distribuion Function
    \[
        F_X(x_i) = P(X \le x_i)  
    \]
    \begin{center}
        \img{./img/CDF2.png}
    \end{center}
    Additionally,
    \[
        F_X(x_i) = P(X \le x_i) = \int_{-\infty}^{x_i} f_X(u)du
    \]

    and it follows from the Fundamental Theorem of Calculus that
    \[
        f_X(x) = \frac{d}{dx}F_X(x)
    \]
\end{enumerate}

\subsubsection*{Rules of Continuous Random Variables}
\begin{itemize}
    \item $f_X(x) \ge 0$
    \item $\int_{-\infty}^{+\infty}f_X(x)dx = 1 = S$
    \item $F_X(-\infty) = 0$
    \item $F_X({}\infty) = 1$ 
    \item $F_X(b) \ge F_X(a) \text{ if } b \ge a$
\end{itemize}

Some of these rules hold in both the discrete and the continuous case.

\section{Joint Random Variables}

Sometime it is useful to consider the probabilistic relationship between two random variables. This is called the joint probability. Just as before, we can use 2 methods to describe the joint distribution of continuous random variables.
\begin{enumerate}
    \item Joint Probability Density Function
    \[
        f_{X,Y}(x, y)dxdy = P(x < X \le x + dx \cap y < Y \le y + dy) 
    \]
    Similarly to single PDF's, we can find the probability of X and Y within a certain region as follows.
    \[
        P(a < X \le b \cap c < Y \le d) = \int_a^b\int_c^d f_{X, Y}(u, v)dudv
    \]  

    Two conditions must hold for joint PDF's
    \begin{enumerate}
        \item $f_{X, Y}(x, y) \ge 0$
        \item $\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} f_{X, Y}(u, v)dudv$
    \end{enumerate}

    \vspace{1cm}

    \item Joint Cumulative Distribution Function
    \[
        F_{X, Y}(x, y) = P(X \le x \cap Y \le y) = \int_{-\infty}^{x}\int_{-\infty}^y f_{X, Y}(u, v)dudv
    \]
    And just as in the single variable case, the Fundamental Theoreom of Calculus can be applied to obtain  
    \[
        f_{X,Y}(x, y) = \frac{\delta^2}{\delta x\delta y} F_{X,Y} (x, y)
    \]  
\end{enumerate}

\subsubsection*{Marginal Distributions}
Given the joint distribution of X and Y, we can obtain the distributions of X alone or Y alone. This is called the marginal distribution.

\[
    f_X(x) = \int_{-\infty}^{+\infty}f_{X, Y}(x, y)dy
\]

This follows from the fact that the the distribution of y from $-\infty$ to $+\infty$ is 1 and the intersection of any value with 1 is the original value. 

The same style of manipulation can be performed in order to obtain the marginal CDF.

\[
    F_X(x) = F_X(x, \infty)
\]

\subsubsection*{Conditional Probability Distributions}
Conditional probability distributions are used to determine the probability of a variable given the value of another variable. Put another way, $P(X | Y)$.

For continuous random variables, the conditional PDF is
\[
    f_{X|Y}(x|y) = P(X = x | Y = y) = \frac{f_{X,Y}(x, y)}{f_Y(y)}
\]

The conditional CDF is
\[
    F_{X|Y}(x|y) = P(X \le x | Y = y) = \int_{-\infty}^{x}f_{X|Y}(u|y)du 
\]

\subsubsection*{Independence}
Similar independence rules hold for random variables as in the case of events.

Specifically, X and Y are said to be independent if
\[
    f_{X|Y}(x| y) = f_X(x) \text{   }\forall y
\]

The following are equivalent to the above statement
\begin{itemize}
    \item $f_{Y|X}(y|x) = f_Y(y)$
    \item $f_{X,Y}(x, y) = f_X(x)f_Y(y)$
    \item $F_{X|Y}(x| y) = F_X(x) $
    \item $F_{Y|X}(y|x) = F_Y(y)$
    \item $F_{X,Y}(x, y) = F_X(x)F_Y(y)$
\end{itemize}

We often assume independence in order to simplify calculations.

\subsection*{Joint Discrete Random Variables}
The rules for joint distributions also apply to discrete random variables. 

Specifically,

\[
  p_{X, Y} = P(X = x \cap Y = y)
\]

\[
    p_{X|Y} = \frac{p_{X,Y}(x, y)}{p_Y(y)}
\]

\[
    p_X(x) = \sum_{\text{all } y_i} p_{X, Y}(x, y_i)
\]

\[
    p_{X|Y}(x|y) = p_X(X) \text{ given X and Y are independent}
\]